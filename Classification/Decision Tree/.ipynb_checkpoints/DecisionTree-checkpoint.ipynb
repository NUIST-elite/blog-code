{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1fe5ae-4bb8-4617-95e9-42d39c2a82a0",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "This notebook is a summary on the usage of **decision tree** algorithm package in **scikit-learn**. \n",
    "\n",
    "Two main class for decision tree is `DecisionTreeClassifier` and `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1cc3d6-e1b2-4bbd-b28e-0162c6351222",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier\n",
    "**DecisionTreeClassifier** is a class capable of performing multi-class classification on a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3d3113-ba19-40d0-b681-63c9ccb960a3",
   "metadata": {},
   "source": [
    "### Main parameters\n",
    "Here are some introduction for main paranmeters in **DecisionTreeClaasifier**.\n",
    "\n",
    "#### criterion\n",
    "The function to measure the quality of a split. \n",
    "\n",
    "Either **\"gini\"** or **\"entropy\"** can be used, the former representing the Gini coefficient and the latter representing the information gain. Default **\"gini\"**.\n",
    "\n",
    "#### splitter\n",
    "The strategy used to choose the split at each node. \n",
    "\n",
    "Supported strategies are **\"best\"** to choose the best split and **\"random\"** to choose the best random split. Default **\"best\"**. \n",
    "\n",
    "The default **\"best\"** is suitable when the sample size is small, and if the sample data size is very large, **\"random\"** is recommended.\n",
    "\n",
    "#### max_features\n",
    "The number of features to consider when looking for the best split.\n",
    "\n",
    "Many types of values can be used, the default is **\"None\"**, which means that all features are considered when dividing.\n",
    "\n",
    "if it is **\"log2\"**, it means that at most $log_2N$ features are considered when dividing\n",
    "\n",
    "if it is **\"sqrt\"** or **\"auto\"** means at most $\\sqrt{N}$ features are considered when dividing. \n",
    "\n",
    "If it is an **integer**, it represents the absolute number of features under consideration. If it is a **float**, it represents the percentage of features considered, that is, the number of features after rounding is considered. where N is the total number of features of the sample.\n",
    "\n",
    "Generally speaking, if the number of sample features is small, such as less than 50, we can use the default **\"None\"**. If the number of features is very large, we canuse the other values just described to control the largest feature considered when dividing.\n",
    "\n",
    "#### max_depth\n",
    "The maximum depth of the tree. Default None.\n",
    "\n",
    "This value can be ignored when there are few data or features. If the model has a large sample size and a large number of features, it is recommended to limit the maximum depth. The specific value depends on the distribution of the data. **Commonly used values can be between 10-100**.\n",
    "\n",
    "#### min_samples_split\n",
    "The minimum number of samples required to split an internal node. Default 2.\n",
    "\n",
    "If the sample is relatively small, this value can be ignored. If the sample is very large, this paramter is recommended. For example, set to 10 for 100 thousands sample.\n",
    "\n",
    "#### min_samples_leaf\n",
    "The minimum number of samples required to be at a leaf node. Default 1.\n",
    "\n",
    "A split point at any depth will only be considered if it leaves at least `min_samples_leaf` training samples in each of the left and right branches.\n",
    "\n",
    "If the sample is relatively small, this value can be ignored. If the sample is very large, this paramter is recommended. For example, set to 5 for 100 thousands sample.\n",
    "\n",
    "#### min_weight_fraction_leaf\n",
    "The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Default 0.0.\n",
    "\n",
    "Normally it is not used. However, if we have samples with lots of **missing values**, or the distribution category of the classification tree samples has a **large deviation**, the sample weight will be introduced, and then we must pay attention to this value.\n",
    "\n",
    "#### max_leaf_nodes\n",
    "Overfitting can be prevented by limiting the maximum number of leaf nodes. Default None.\n",
    "\n",
    "If the limit is added, the algorithm will build the optimal decision tree within the maximum number of leaf nodes. \n",
    "\n",
    "If there are not many features, this value can be ignored, but if there are many features, it can be limited, and the specific value can be obtained through cross-validation.\n",
    "\n",
    "#### class_weight\n",
    "Using a dictionary to specify weight for each class. Default None.\n",
    "\n",
    "The main purpose is to prevent **too many samples of certain classes** in the training set, which will cause the trained decision tree to be too **biased** towards these classes. **\"balanced\"** can be used to let sklearn calculate weight for you.\n",
    "\n",
    "#### min_impurity_split\n",
    "A node will be split if this split induces a decrease of the impurity greater than or equal to this value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb327a6-35f9-401c-8c63-07d5c1bae614",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor\n",
    "Most parameters is the same as `DecisionTreeClassifier`, except the following:\n",
    "1. **criterion**: {“squared_error”, “friedman_mse”, “absolute_error”, “poisson”}, default=”squared_error”.\n",
    "2. **class_weight** is not supported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1f783a-13eb-4591-98ca-408a98f15161",
   "metadata": {},
   "source": [
    "## Example\n",
    "We will generate some classification data which is reletively large to inspect the usage of each parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812d9f04-7851-4d91-8388-4f4220003258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_classification(n_samples=, n_features=2, n_redundant=0, n_classes=3, n_clusters_per_class=1)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0efd8d0-2e19-40fb-b2dd-5a20c114644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "clf = DecisionTreeClassifier()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
